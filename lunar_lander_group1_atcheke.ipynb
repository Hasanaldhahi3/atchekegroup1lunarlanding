{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlacodes/atchekegroup1lunarlanding/blob/sherry/lunar_lander_group1_atcheke.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "rKjkh_KgobbF"
      },
      "source": [
        "# Performance Analysis of DQN Algorithm on the Lunar Lander task\n",
        "\n",
        "**By Neuromatch Academy**\n",
        "\n",
        "__Content creators:__ Raghuram Bharadwaj Diddigi, Geraud Nangue Tasse, Yamil Vidal, Sanjukta Krishnagopal, Sara Rajaee\n",
        "\n",
        "__Content editors:__ Spiros Chavlis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "iDR5t137obbP"
      },
      "source": [
        "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "brZ9ZJQzobbR"
      },
      "source": [
        "---\n",
        "# Objective\n",
        "\n",
        "In this project, the objective is to analyze the performance of the Deep Q-Learning algorithm on an exciting task- Lunar Lander. Before we describe the task, let us focus on two keywords here - analysis and performance. What exactly do we mean by these keywords in the context of Reinforcement Learning (RL)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "wpX-jA7GobbT"
      },
      "source": [
        "---\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {},
        "id": "9EanvrrhobbU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87929ee4-bce9-49bf-b2c1-9547659b763f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 177 kB 9.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 42.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 37.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 49.4 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 448 kB 5.7 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# @title Install dependencies\n",
        "!sudo apt-get update > /dev/null 2>&1\n",
        "!sudo apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install rarfile --quiet\n",
        "!pip install stable-baselines3[extra] ale-py==0.7.4 --quiet\n",
        "!pip install box2d-py --quiet\n",
        "!pip install gym pyvirtualdisplay --quiet\n",
        "c"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install optuna --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSg7t4EJx9Pp",
        "outputId": "634a6365-cebf-416d-dc93-9cac84635c82"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 308 kB 8.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 209 kB 47.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 81 kB 7.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 147 kB 50.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 49.2 MB/s \n",
            "\u001b[?25h  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "execution": {},
        "id": "pE3qZJLcobbW"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import base64\n",
        "import stable_baselines3\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.results_plotter import ts2xy, load_results\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.wrappers import Monitor,RecordVideo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gym --version"
      ],
      "metadata": {
        "id": "gPoZnSTGFvEp",
        "outputId": "0f3e1d2d-cc36-4a24-9096-3835971d2e7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: gym: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import sys"
      ],
      "metadata": {
        "id": "CHT0LZ0WyERZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "execution": {},
        "id": "fp1bUnClobbY"
      },
      "outputs": [],
      "source": [
        "# @title Plotting/Video functions\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment\n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  # env = RecordVideo(env, './video')\n",
        "  return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "psfdqUCmobba"
      },
      "source": [
        "---\n",
        "# Introduction\n",
        "\n",
        "In a standard RL setting, an agent learns optimal behavior from an environment through a feedback mechanism to maximize a given objective. Many algorithms have been proposed in the RL literature that an agent can apply to learn the optimal behavior. One such popular algorithm is the Deep Q-Network (DQN). This algorithm makes use of deep neural networks to compute optimal actions. In this project, your goal is to understand the effect of the number of neural network layers on the algorithm's performance. The performance of the algorithm can be evaluated through two metrics - Speed and Stability. \n",
        "\n",
        "**Speed:** How fast the algorithm reaches the maximum possible reward. \n",
        "\n",
        "**Stability** In some applications (especially when online learning is involved), along with speed, stability of the algorithm, i.e., minimal fluctuations in performance, is equally important. \n",
        "\n",
        "In this project, you should investigate the following question:\n",
        "\n",
        "**What is the impact of number of neural network layers on speed and stability of the algorithm?**\n",
        "\n",
        "You do not have to write the DQN code from scratch. We have provided a basic implementation of the DQN algorithm. You only have to tune the hyperparameters (neural network size, learning rate, etc), observe the performance, and analyze. More details on this are provided below. \n",
        "\n",
        "Now, let us discuss the RL task we have chosen, i.e., Lunar Lander. This task consists of the lander and a landing pad marked by two flags. The episode starts with the lander moving downwards due to gravity. The objective is to land safely using different engines available on the lander with zero speed on the landing pad as quickly and fuel efficient as possible. Reward for moving from the top of the screen and landing on landing pad with zero speed is between 100 to 140 points. Each leg ground contact yields a reward of 10 points. Firing main engine leads to a reward of -0.3 points in each frame. Firing the side engine leads to a reward of -0.03 points in each frame. An additional reward of -100 or +100 points is received if the lander crashes or comes to rest respectively which also leads to end of the episode. \n",
        "\n",
        "The input state of the Lunar Lander consists of following components:\n",
        "\n",
        "  1. Horizontal Position\n",
        "  2. Vertical Position\n",
        "  3. Horizontal Velocity\n",
        "  4. Vertical Velocity\n",
        "  5. Angle\n",
        "  6. Angular Velocity\n",
        "  7. Left Leg Contact\n",
        "  8. Right Leg Contact\n",
        "\n",
        "The actions of the agents are:\n",
        "  1. Do Nothing\n",
        "  2. Fire Main Engine\n",
        "  3. Fire Left Engine\n",
        "  4. Fire Right Engine\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/projects/static/lunar_lander.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "fGCTNteSobbd"
      },
      "source": [
        "---\n",
        "# Basic DQN Implementation\n",
        "\n",
        "We will now implement the DQN algorithm using the existing code base. We encourage you to understand this example and re-use it in an application/project of your choice! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "sEG8U0Pmobbf"
      },
      "source": [
        "Now, let us set some hyperparameters for our algorithm. This is the only part you would play around with, to solve the first part of the project. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning hyperparameters"
      ],
      "metadata": {
        "id": "XgNuDz2mrq0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ref: \n",
        "\n",
        "1.   template https://github.com/AI4Finance-Foundation/FinRL/blob/master/tutorials/4-Optimization/FinRL_HyperparameterTuning_using_Optuna_basic.ipynb\n",
        "\n",
        "2.   tutorial (combined with template): https://medium.com/analytics-vidhya/hyperparameter-tuning-using-optuna-for-finrl-8a49506d2741\n",
        "\n",
        "3.  set parameter function: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/utils/hyperparams_opt.py\n",
        "\n",
        "4.   callback function: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/utils/callbacks.py#L10\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6RQ_bAC6KlAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define search space"
      ],
      "metadata": {
        "id": "i_gZFwkKtj5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_dqn_params(trial: optuna.Trial):\n",
        "    \"\"\"\n",
        "    Sampler for DQN hyperparams.\n",
        "    :param trial:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
        "\n",
        "    net_arch = trial.suggest_categorical(\"net_arch\", [\"two\", \"three\"]) #, \"two_big\"\n",
        "    net_arch = {\"two\": [64, 64], \"three\": [64, 64, 64]}[net_arch] #, \"two_big\": [256, 256]\n",
        "\n",
        "    hyperparams = {\n",
        "        \"learning_rate\": learning_rate,#\n",
        "        \"policy_kwargs\": dict(activation_fn=torch.nn.ReLU, net_arch=net_arch),\n",
        "        \"batch_size\": 1,  #for simplicity, we are not doing batch update.\n",
        "        \"buffer_size\": 1, #size of experience of replay buffer. Set to 1 as batch update is not done\n",
        "        \"learning_starts\": 1, #learning starts immediately!\n",
        "        \"gamma\": 0.99, #discount facto. range is between 0 and 1.\n",
        "        \"tau\": 1,  #the soft update coefficient for updating the target network\n",
        "        \"target_update_interval\": 1, #update the target network immediately.\n",
        "        \"train_freq\": (1,\"step\"), #train the network at every step.\n",
        "        \"max_grad_norm\": 10, #the maximum value for the gradient clipping\n",
        "        \"exploration_initial_eps\": 1, #initial value of random action probability\n",
        "        \"exploration_fraction\": 0.5, #fraction of entire training period over which the exploration rate is reduced\n",
        "        \"gradient_steps\": 1, #number of gradient steps\n",
        "        \"seed\": 1, #seed for the pseudo random generators\n",
        "        \"verbose\": 0\n",
        "    }\n",
        "    return hyperparams"
      ],
      "metadata": {
        "id": "mWhSf_qjk8T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define callback"
      ],
      "metadata": {
        "id": "fFyJprz6tp7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LoggingCallback:\n",
        "    def __init__(self,threshold,trial_number,patience):\n",
        "      '''\n",
        "      threshold: int tolerance for increase in reward\n",
        "      trial_number: int Prune after minimum number of trials\n",
        "      patience: int patience for the threshold\n",
        "      '''\n",
        "      self.threshold = threshold\n",
        "      self.trial_number  = trial_number\n",
        "      self.patience = patience\n",
        "      self.cb_list = [] #Trials list for which threshold is reached\n",
        "    def __call__(self,study:optuna.study, frozen_trial:optuna.Trial):\n",
        "      #Setting the best value in the current trial\n",
        "      study.set_user_attr(\"previous_best_value\", study.best_value)\n",
        "      \n",
        "      #Checking if the minimum number of trials have pass\n",
        "      if frozen_trial.number >self.trial_number:\n",
        "          previous_best_value = study.user_attrs.get(\"previous_best_value\",None)\n",
        "          #Checking if the previous and current objective values have the same sign\n",
        "          if previous_best_value * study.best_value >=0:\n",
        "              #Checking for the threshold condition\n",
        "              if abs(previous_best_value-study.best_value) < self.threshold: \n",
        "                  self.cb_list.append(frozen_trial.number)\n",
        "                  #If threshold is achieved for the patience amount of time\n",
        "                  if len(self.cb_list)>self.patience:\n",
        "                      print('The study stops now...')\n",
        "                      print('With number',frozen_trial.number ,'and value ',frozen_trial.value)\n",
        "                      print('The previous and current best values are {} and {} respectively'\n",
        "                              .format(previous_best_value, study.best_value))\n",
        "                      study.stop()"
      ],
      "metadata": {
        "id": "Q4FKnTdKttOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define objective"
      ],
      "metadata": {
        "id": "B1qV393bpoTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = \"models\"\n",
        "os.makedirs(log_dir,exist_ok=True) \n",
        "\n",
        "def objective(trial:optuna.Trial):\n",
        "\n",
        "  # Create environment\n",
        "  env = gym.make('LunarLander-v2')\n",
        "  env = stable_baselines3.common.monitor.Monitor(env, log_dir)\n",
        "\n",
        "  #Trial will suggest a set of hyperparamters from the specified range\n",
        "  hyperparameters = sample_dqn_params(trial)\n",
        "  model_dqn = DQN(\"MlpPolicy\", env, **hyperparameters) #Set verbose to 1 to observe training logs. We encourage you to set the verbose to 1.\n",
        "\n",
        "  # define learning steps\n",
        "  trained_dqn = model_dqn.learn(total_timesteps=100000, log_interval=10)#100000 , callback=callback\n",
        "  # save model\n",
        "  trained_dqn.save('models/dqn_{}.pth'.format(trial.number)) \n",
        "\n",
        "  x, y = ts2xy(load_results(log_dir), 'episodes') # timesteps\n",
        "  # clear_output(wait=True)\n",
        "  #For the given hyperparamters, determine reward\n",
        "  reward = sum(y)\n",
        "  return reward\n",
        "\n",
        "#Create a study object and specify the direction as 'maximize'\n",
        "#As you want to maximize reward\n",
        "#Pruner stops not promising iterations\n",
        "#Use a pruner, else you will get error related to divergence of model\n",
        "#You can also use Multivariate samplere\n",
        "#sampler = optuna.samplers.TPESampler(multivarite=True,seed=42)\n",
        "sampler = optuna.samplers.TPESampler(seed=42)\n",
        "study = optuna.create_study(study_name=\"dqn_study\",direction='maximize',\n",
        "                            sampler = sampler, pruner=optuna.pruners.HyperbandPruner())\n",
        "\n",
        "logging_callback = LoggingCallback(threshold=10, patience=30, trial_number=5)\n",
        "#You can increase the n_trials for a better search space scanning\n",
        "study.optimize(objective, n_trials=4, catch=(ValueError,),callbacks=[logging_callback])"
      ],
      "metadata": {
        "id": "2s-bjWndt_a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get the best model"
      ],
      "metadata": {
        "id": "tf91UWqvzR6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the best hyperparamters\n",
        "print('Hyperparameters after tuning',study.best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcDma0wSy02u",
        "outputId": "60f8937f-a7ed-4951-9b97-43416b0b9fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters after tuning {'learning_rate': 0.0001329291894316216, 'net_arch': 'two'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_trial"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqou4GOlzWMh",
        "outputId": "c74fe0ff-db2e-499d-9899-354cf2e6da7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenTrial(number=0, values=[-64206.27353400003], datetime_start=datetime.datetime(2022, 7, 20, 8, 56, 57, 127327), datetime_complete=datetime.datetime(2022, 7, 20, 9, 6, 23, 876212), params={'learning_rate': 0.0001329291894316216, 'net_arch': 'two'}, distributions={'learning_rate': LogUniformDistribution(high=0.01, low=1e-05), 'net_arch': CategoricalDistribution(choices=('two', 'three'))}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=0, state=TrialState.COMPLETE, value=None)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Template"
      ],
      "metadata": {
        "id": "mObWM9TLp2lj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "execution": {},
        "id": "vJm_qzWkobbg"
      },
      "outputs": [],
      "source": [
        "nn_layers = [64,64] #This is the configuration of your neural network. Currently, we have two layers, each consisting of 64 neurons.\n",
        "                    #If you want three layers with 64 neurons each, set the value to [64,64,64] and so on.\n",
        "\n",
        "learning_rate = 0.001 #This is the step-size with which the gradient descent is carried out.\n",
        "                      #Tip: Use smaller step-sizes for larger networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "-9bGzUVPobbh"
      },
      "source": [
        "Now, let us setup our model and the DQN algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "execution": {},
        "id": "1dN91ycLobbh"
      },
      "outputs": [],
      "source": [
        "log_dir = \"/tmp/gym/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Create environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "#You can also load other environments like cartpole, MountainCar, Acrobot. Refer to https://gym.openai.com/docs/ for descriptions.\n",
        "#For example, if you would like to load Cartpole, just replace the above statement with \"env = gym.make('CartPole-v1')\".\n",
        "\n",
        "env = stable_baselines3.common.monitor.Monitor(env, log_dir )\n",
        "\n",
        "callback = EvalCallback(env,log_path = log_dir, deterministic=True) #For evaluating the performance of the agent periodically and logging the results.\n",
        "\n",
        "policy_kwargs = dict(activation_fn=torch.nn.ReLU,\n",
        "                     net_arch=nn_layers)\n",
        "model = DQN(\"MlpPolicy\", env, policy_kwargs = policy_kwargs,\n",
        "            learning_rate=learning_rate,\n",
        "            batch_size=1,  #for simplicity, we are not doing batch update.\n",
        "            buffer_size=1, #size of experience of replay buffer. Set to 1 as batch update is not done\n",
        "            learning_starts=1, #learning starts immediately!\n",
        "            gamma=0.99, #discount facto. range is between 0 and 1.\n",
        "            tau = 1,  #the soft update coefficient for updating the target network\n",
        "            target_update_interval=1, #update the target network immediately.\n",
        "            train_freq=(1,\"step\"), #train the network at every step.\n",
        "            max_grad_norm = 10, #the maximum value for the gradient clipping\n",
        "            exploration_initial_eps = 1, #initial value of random action probability\n",
        "            exploration_fraction = 0.5, #fraction of entire training period over which the exploration rate is reduced\n",
        "            gradient_steps = 1, #number of gradient steps\n",
        "            seed = 1, #seed for the pseudo random generators\n",
        "            verbose=0) #Set verbose to 1 to observe training logs. We encourage you to set the verbose to 1.\n",
        "\n",
        "# You can also experiment with other RL algorithms like A2C, PPO, DDPG etc. Refer to  https://stable-baselines3.readthedocs.io/en/master/guide/examples.html\n",
        "#for documentation. For example, if you would like to run DDPG, just replace \"DQN\" above with \"DDPG\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "0-7OLtdpobbi"
      },
      "source": [
        "Before we train the model, let us look at an instance of Lunar Lander **before training**.  \n",
        "\n",
        "**Note:** The following code for rendering the video is taken from https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb#scrollTo=T9RpF49oOsZj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "execution": {},
        "id": "8nEQvdE7obbj",
        "outputId": "1a069bb5-4930-4874-acc2-7d630e09d625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video alt=\"test\" autoplay\n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "test_env = wrap_env(gym.make(\"LunarLander-v2\"))\n",
        "observation = test_env.reset()\n",
        "total_reward = 0\n",
        "while True:\n",
        "  test_env.render()\n",
        "  action, states = model.predict(observation, deterministic=True)\n",
        "  observation, reward, done, info = test_env.step(action)\n",
        "  total_reward += reward\n",
        "  if done:\n",
        "    break;\n",
        "\n",
        "# print(total_reward)\n",
        "test_env.close()\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ZpMABSl9obbk"
      },
      "source": [
        "From the video above, we see that the lander has crashed! \n",
        "It is now the time for training! \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {},
        "id": "I0hzLf1lobbl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23820c62-3e1d-4c0d-bae0-376dad1b87ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.dqn.dqn.DQN at 0x7f1a38044350>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model.learn(total_timesteps=1000, log_interval=10, callback=callback, eval_log_path=log_dir)\n",
        "# The performance of the training will be printed every 10 episodes. Change it to 1, if you wish to\n",
        "# view the performance at every training episode.\n",
        "# how to read monitor.csv: ep_info = {\"r\": round(ep_rew, 6), \"l\": ep_len, \"t\": round(time.time() - self.t_start, 6)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ueR5BIcpobbm"
      },
      "source": [
        "The training takes time. We encourage you to analyze the output logs (set verbose to 1 to print the output logs). The main component of the logs that you should track is \"ep_rew_mean\" (mean of episode rewards). As the training proceeds, the value of \"ep_rew_mean\" should increase. The improvement need not be monotonic, but the trend should be upwards! \n",
        "\n",
        "Along with training, we are also periodically evaluating the performance of the current model during the training. This was reported in logs as follows:\n",
        "\n",
        "```\n",
        "Eval num_timesteps=100000, episode_reward=63.41 +/- 130.02\n",
        "Episode length: 259.80 +/- 47.47\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ogkoKXiXobbm"
      },
      "source": [
        "Now, let us look at the visual performance of the lander. \n",
        "\n",
        "**Note:** The performance varies across different seeds and runs. This code is not optimized to be stable across all runs and seeds. We hope you will be able to find an optimal configuration! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "m_WCp8wdobbn",
        "outputId": "3fdda593-41fc-4f87-de3b-d00ab69b0bbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1b69d83af4b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LunarLander-v2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wrap_env' is not defined"
          ]
        }
      ],
      "source": [
        "env = wrap_env(gym.make(\"LunarLander-v2\"))\n",
        "observation = env.reset()\n",
        "while True:\n",
        "  env.render()\n",
        "  action, _states = model.predict(observation, deterministic=True)\n",
        "  observation, reward, done, info = env.step(action)\n",
        "  if done:\n",
        "    break;\n",
        "\n",
        "env.close()\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Ov5djtn9obbn"
      },
      "source": [
        "The lander has landed safely!!\n",
        "\n",
        "Let us analyze its performance (speed and stability). For this purpose, we plot the number of time steps on the X-axis and the episodic reward given by the trained model on the Y-axis. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "asuy5ryIobbo",
        "outputId": "63569cc2-a0c7-464a-af73-e11b361c24cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Episode Rewards')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEKCAYAAADuEgmxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5hU5fn/8fe9y9LLgvSlFwvSWRBbxBa7GDUKiTRRrNFEE7/6NTHRaH4m5qsJVhBBwILGElHBji0qsMBKJyy99yp1d+/fH3NIVoTlADtzZnc+r+uai5nnnJlzMxfsZ59zzvM85u6IiIiEkRZ1ASIiUnooNEREJDSFhoiIhKbQEBGR0BQaIiISmkJDRERCiyw0zKyimU0ys2/NbJaZ3R+0NzeziWaWZ2avmFn5oL1C8Dov2N4sqtpFRFJVlD2N3cBZ7t4B6Aicb2bdgT8Dj7l7K2ATMDDYfyCwKWh/LNhPREQSKLLQ8JjtwcuM4OHAWcBrQftI4LLgec/gNcH2s83MElSuiIgA5aI8uJmlA1OAVsCTwAJgs7vnB7ssB7KC51nAMgB3zzezLcAxwPr9PnMQMAigSpUqXY4//vh4/zVERMqUKVOmrHf3OgfaFmlouHsB0NHMMoE3gaP+Ce/uQ4GhANnZ2Z6Tk3O0HykiklLMbMnBtiXF3VPuvhmYAJwMZJrZvjBrBKwInq8AGgME22sAGxJcqohISovy7qk6QQ8DM6sEnAvMIRYeVwa79QPeCp6PDV4TbP/ENduiiEhCRXl6qgEwMriukQa86u7vmNlsYIyZPQhMA54L9n8OGG1mecBGoFcURYuIpLLIQsPdpwOdDtC+EOh2gPZdwE8TUJqIiBxEUlzTEBGR0kGhISIioSk0REQkNIWGiIiEptAQEZHQFBoiIhKaQkNEREJTaIiISGgKDRERCU2hISIioSk0REQkNIWGiIiEptAQEZHQFBoiIhKaQkNEREJTaIiISGgKDRERCU2hISIioSk0REQkNIWGiIiEptAQEZHQFBoiIhKaQkNEREJTaIiISGgKDRERCU2hISIioSk0REQktMhCw8wam9kEM5ttZrPM7PagvZaZfWhm84M/awbtZmaDzSzPzKabWeeoahcRSVVR9jTygTvdvQ3QHbjFzNoAdwMfu3tr4OPgNcAFQOvgMQh4OvEli4iktshCw91XufvU4Pk2YA6QBfQERga7jQQuC573BEZ5zDdAppk1SHDZIiIpLSmuaZhZM6ATMBGo5+6rgk2rgXrB8yxgWZG3LQ/aREQkQSIPDTOrCrwO/NLdtxbd5u4O+GF+3iAzyzGznHXr1pVgpSIiEmlomFkGscB40d3fCJrX7DvtFPy5NmhfATQu8vZGQdv3uPtQd8929+w6derEr3gRkRQU5d1TBjwHzHH3R4tsGgv0C573A94q0t43uIuqO7ClyGksERFJgHIRHvtUoA8ww8xyg7b/BR4GXjWzgcAS4Kpg2zjgQiAP2AEMSGy5IiISWWi4+5eAHWTz2QfY34Fb4lqUiIgUK/IL4SIiUnooNEREJDSFhoiIhKbQEBGR0BQaIiISmkJDRERCU2iIiEhoCg0REQlNoSEiIqEpNEREJDSFhoiIhKbQEBGR0BQaIiISmkJDRERCU2iIiEhoCg0REQlNoSEiIqEpNEREJDSFhoiIhKbQEBGR0BQaIiISmkJDRERCU2iIiEhoCg0REQlNoSEiIqEpNEREJLTDCg0zSzOz6vEqRkREktshQ8PMXjKz6mZWBZgJzDaz38S/NBERSTZhehpt3H0rcBkwHmgO9CmJg5vZcDNba2Yzi7TVMrMPzWx+8GfNoN3MbLCZ5ZnZdDPrXBI1iIhIeGFCI8PMMoiFxlh33wt4CR3/eeD8/druBj5299bAx8FrgAuA1sFjEPB0CdUgIiIhhQmNIcBioArwuZk1BbaWxMHd/XNg437NPYGRwfORxMJqX/soj/kGyDSzBiVRh4iIhHPI0HD3we6e5e4XBj+wlwBnxrGmeu6+Kni+GqgXPM8ClhXZb3nQ9j1mNsjMcswsZ926dXEsU0Qk9ZQ72AYzu+MQ7320hGv5AXd3MzusU2HuPhQYCpCdnV1Sp9FERIRiQgOoFvx5HNAVGBu8vgSYFMea1phZA3dfFZx+Whu0rwAaF9mvUdAmIiIJctDTU+5+v7vfT+yHc2d3v9Pd7wS6AE3iWNNYoF/wvB/wVpH2vsFdVN2BLUVOY4mISAIU19PYpx6wp8jrPfz3OsNRMbOXgR5AbTNbDvweeBh41cwGAkuAq4LdxwEXAnnADmBASdQgIiLhhQmNUcAkM3szeH0ZsVtlj5q79z7IprMPsK8Dt5TEcUVE5MgUGxpmZsRCYzxwetA8wN2nxbswERFJPsWGRnD30jh3bwdMTVBNIiKSpMIM7ptqZl3jXomIiCS9MNc0TgJ+bmZLgO8AI9YJaR/XykREJOmECY3z4l6FiIiUCocMjWDaEMysLlAx7hWJiEjSCrOexqVmNh9YBHxGbPLC8XGuS0REklCYC+F/BLoD/3b35sTGUHwT16pERCQphQmNve6+AUgzszR3nwBkx7kuERFJQmEuhG82s6rA58CLZraW2F1UIiKSYsL0NHoSm+vpV8B7wAJiM92KiEiKCdPT6AV87u7z+e+KeiIikoLChEYTYIiZNQdyiJ2m+sLdc+NamYiIJJ0wy73+3t3PAtoAXwC/AabEuzAREUk+h+xpmNlvgVOBqsA04NfEwkNERFJMmNNTlwP5wLvEBvd97e6741qViIgkpTCnpzoD5xBbF/xcYIaZfRnvwkREJPmEOT3VltgCTGcQG9S3DJ2eEhFJSWFOTz1MLCQGA5PdfW98SxIRkWQVZpbbi82sEtBEgSEiktrCzHJ7CZBLbDQ4ZtbRzMbGuzAREUk+YaYR+QPQDdgMEAzqax7HmkREJEmFneV2y35tHo9iREQkuYW5ED7LzH4GpJtZa+A24Kv4liUiIskoTE/jF8CJwG7gZWALcHs8ixIRkeQUZnDfDne/1927uns2MBp4Iv6liYhIsjloaJhZezP7wMxmmtmDZtbAzF4HPgZmJ65EERFJFsX1NJ4FXgKuANYTu+12AdDK3R9LQG0HZGbnm9k8M8szs7ujqkNEJBUVFxoV3P15d5/n7n8DvnP3u9x9V6KK25+ZpQNPAhcQm6q9t5m1iaoeEZFUU9zdUxXNrBNgwevdRV+7+9R4F3cA3YA8d18IYGZjiC1Hq9NlIiIJUFxorAIeLfJ6dZHXDpwVr6KKkUVswsR9lgMnFd3BzAYBgwCaNGmSuMpERFLAQUPD3c9MZCElxd2HAkMBsrOzNQhRRKQEhRmnkUxWAI2LvG4UtImISAKUttCYDLQ2s+ZmVh7oBWjyRBGRBAkzjUjScPd8M7sVeB9IB4a7+6yIyxIRSRlhVu4z4OdAC3d/wMyaAPXdfVLcqzsAdx8HjIvi2CIiqS7M6amngJOB3sHrbcTGSoiISIoJc3rqJHfvbGbTANx9U3A9QUREUkyo9TSCkdgOYGZ1gMK4ViUiIkkpTGgMBt4E6prZQ8CXwJ/iWpWIiCSlQ56ecvcXzWwKcDaxKUQuc/c5ca9MRESSzkFDw8xqFXm5ltgCTP/Z5u4b41mYiIgkn+J6GlOIXccwoAmwKXieCSwFmse9OhERSSoHvabh7s3dvQXwEXCJu9d292OAi4EPElWgiIgkjzAXwrsHA+oAcPfxwCnxK0lERJJVmHEaK83st8ALweufAyvjV5KIiCSrMD2N3kAdYrfdvgnU5b+jw0VEJIWEueV2I3C7mVWLvfTt8S9LRESS0SF7GmbWLphCZCYwy8ymmFnb+JcmIiLJJszpqSHAHe7e1N2bAncSrIwnIiKpJUxoVHH3CfteuPunQJW4VSQiIkkrzN1TC83sd8Do4PU1wML4lSQiIskqTE/jWmJ3T70RPGoHbSIikmLC3D21CbgNIJgivYq7b413YSIiknzC3D31kplVN7MqwAxgtpn9Jv6liYhIsglzeqpN0LO4DBhPbKLCPnGtSkREklKY0MgwswxioTHW3fcSrOInIiKpJew4jcXEbrP93MyaArqmISKSgsJcCB9MbMnXfZaY2ZnxK0lERJJVcSv3XePuL5jZHQfZ5dE41SQiIkmquJ7GvlHf1RJRiIiIJL+Dhoa7Dwn+vD9x5Ygc3KotO1m/bQ/tGtWIuhSRlBVmnEYLM3vbzNaZ2Voze8vMWiSiOJF91m/fzZVPf83lT/+LnMUboy5HJGWFuXvqJeBVoAHQEPgH8PLRHNTMfmpms8ys0Myy99t2j5nlmdk8MzuvSPv5QVuemd19NMeX0mVPfiE3vzCV9dt3U696RW58YQorNu+MuiyRlBQmNCq7+2h3zw8eLwAVj/K4M4HLgc+LNppZG6AXcCJwPvCUmaUH05c8CVwAtAF6B/tKCnjgnVlMWryRv1zZnucHdGX33kIGjcph556CqEsTSTlhQmO8md1tZs3MrKmZ3QWMM7NaZlbrSA7q7nPcfd4BNvUExrj7bndfBOQB3YJHnrsvdPc9wJhgXynjXpy4hBe+WcoNZ7SgZ8csWtWtxuDenZi9aiu/ee1b3DXOVCSRwoTGVcANwATgU+AmYr2BKUBOCdeTBSwr8np50Haw9h8ws0FmlmNmOevWrSvh8iSRJi3ayO/fmsUZx9bhrvOO/0/7mcfX5a7zjued6at46tMFEVYYnYJCZ+jnC5i5YkvUpUiKCTO4r/mRfLCZfQTUP8Cme939rSP5zDDcfSjByoLZ2dn6NbSUWrF5Jze9MIXGtSozuFcn0tPse9tvPKMFc1dv5a8fzOO4etU4p029iCpNPHfnd2/N5KWJS6lRKYPXbzqZVnV1Z7wkxkF7GsFpqH3Pf7rftj8d6oPd/Rx3b3uAR3GBsQJoXOR1o6DtYO1SBu3cU8ANo3PYnV/Is327UKNyxg/2MTP+fEV72mXV4PYx0/j3mm0RVBqNv34wj5cmLqV3t8ZkpBv9hk9mzdZdUZclKaK401O9ijy/Z79t58ehFoCxQC8zq2BmzYHWwCRgMtDazJqbWfmgtrFxqkEi5O7c/cZ0Zq3cyt+u7ljsb9AVM9IZ0qcLlcqX4/pROWzesSeBlUZj2BcLeXLCAnp3a8yfftKOEf27sWnHHvqPmMy2XXujLk9SQHGhYQd5fqDXh8XMfmJmy4GTgXfN7H0Ad59F7Pbe2cB7wC3uXuDu+cCtwPvAHODVYF8pY4Z+vpC3clfy6x8fF+qUU4MalRjSpwurNu/i1pemkV9QmIAqo/GPnGU8+O4cLmxXnwcva4eZ0a5RDZ76eWfmr9nGjS9MYU9+2f37S3IoLjT8IM8P9PqwuPub7t7I3Su4ez13P6/ItofcvaW7H+fu44u0j3P3Y4NtDx3N8SU5fTpvLQ+/N5eL2jXg5h4tQ7+vS9OaPPiTtnyZt56Hxs2JY4XR+WDWau5+YwantarNY1d3/N41nh7H1eXhK9rzr7wN3PXatxQW6lKexE9xF8I7mNlWYr2KSsFzgtdHO05D5HsWrtvOL16exvH1q/PIT9tjdnid2auyGzN31TaG/2sRJ9SvzlVdGx/6TaXE1ws2cOvL02ibVYMhfbpQoVz6D/a5sksj1mzdxSPvz6NejYrcc8EJEVQqqaC4uad++C9TJA627trL9aNyyEhPY2ifLlQuf8ib+g7ofy88nvlrt3HvP2fQsm4VujQ9omFESWXG8i1cPyqHprUq83z/rlSpcPDv5uYeLVm1ZSdDPltI/eoVGXDqEd34KFKsMOM0ROKmoND55ZhclmzYwVM/70zjWpWP+LPKpafxeO9OZGVW4obRU1lZyqcaWbBuO/1GTKJGpQxGDzyJmlXKF7u/mXH/pW35cZt6PPDObMbNWJWgSiWVKDQkUo9+OI9P5q7l95e0oXuLY4768zIrl2dYv2x27S1g0OjSO9XIys076TNsIgaMHtiN+jXCnRFOTzMG9+5E5yY1+eUruUxcuCG+hUrKUWhIZN7+duV/bh+9pnvTEvvcVnWr8fdeHZm1cit3vT691E01svG7PfR5biLbduUz8tputKhT9bDeXzEjnWF9s2lcsxLXj8pJqTEsEn8KDYnErJVb+M1r39KlaU3uv7TtYV/4PpSzT6jHb847jre/XcnTn5WeqUa2786n/4hJLN+0k2H9smmbdWRrh9SsUp6R13ajYkY6/YZPYtWW0n2qTpKHQkMSbsP23QwaNYXMSuV5+prOlC8Xn3+GN53Rkks7NOSR9+fx0ew1cTlGSdq1t4BBo3KYtXIrT/6sMycd5em6RjUrM2JAV7btyqf/8Mls2anBf3L0FBqSUHsLCrnpxdjaGEP7dqFutfjdvb1vqpETG1bnl6/kMj+JT9PkFxRy+5hpfLVgA49c2b7E5tI6sWHsNt2F67cHU7OUzms8kjwUGpJQD7w9m0mLNvLwFe1o3ygz7serVD6doX2yqZiRnrRTjbg79745k/dnreG+i9tweedGJfr5p7aqzSNXduCbhRu541UN/pOjo9CQhHl50lJGf7OEQT9qwU86lewPxuI0zKzEkD6dWZmkU408PH4ur+Qs47azWnHtafEZW3FZpyzuueB43p2+qsyOmpfEUGhIQuQs3sh9b83k9Na1+Z/zjz/0G0pYl6a1ePCy2FQjfxo3N+HHP5inP13AkM8X0qd7U3517rFxPdagH7Wg/ynNeO7LRQz7YmFcjyVl15ENvRU5DCs37+TGF6aSlVmJJ3p3/sHaGIlyVdfGzF61leH/WsTxDapxVXa0U428PGkpf35vLpd0aMj9l55Y4neQ7c/M+N3FbVi7bRcPvjuHutUrcmmHhnE9ppQ96mlIXO3aW8ANo6ewa28Bz/bNPuDaGIn024tO4LRWtfntmzOZsmRTZHWMn7GKe9+cwRnH1uH/ftqBtAQFaXqa8ehVHenWvBZ3vprLV3nrE3JcKTsUGhI37s7dr09n5sot/O3qjrSuF/3qcuXS03jiZ51okFmRG0ZPiWT8wpfz13P7mFw6NanJM9d0idstxwdTMSOdZ/tk07x2FW4YPYU5q7Ye+k0iAYWGxM2zXyzkn7krufPcY5NqOdbMyuV5tm82O/fkM2hUrBeUKLnLNjNodA4t6lRheL+uVCofzbygNSpn8PyAblSpUI7+IyaxopTP0yWJo9CQuPh03loeHh9bG+OWM1tFXc4PHFuvGn/v1YmZK7dw12uJmWpk/ppt9B8xidpVKzDq2m6Rn6prmFmJ56/tyo49BfQbPikpb0eW5KPQkBK3b22MY+tVO6K1MRLlnDb1+PWPj2Pstyt55rP43k20fNMO+jw3iYz0NF4YeBJ1qyfHkjTH16/O0D7ZLN2wg+tH5SS01yWlk0JDStS2YG2McmnGs32zj3htjES5uUdLLm7fgL+8P5dP5sZnqpH123fT57lJ7NiTz6hru9HkmCOf/j0eTm55DI9e3YHJizfxq1dyKdDgPymGQkNKTGGwNsbiDTt48ijXxkgUM+ORKzvQpkF1bns5l7y1JTvVyNZde/8zYeDw/l05oUH1Ev38knJx+4b87uI2jJ+5mgfenlXqZgaWxFFoSIl59MN/8/Hctdx3cRtOaVk76nJCq1Q+nWf7ZlMxI43rRuawZUfJTOy3a28B143MYd7qbTx9TReymyX3SoIDT2vO9ac3Z+TXSxjyuQb/yYEpNKREvDt9FU9MyOPq7Mb0Pbnk1sZIlIaZlXjmmi6s2LyTW1+eetRTjeQXFHLrS1OZvHgj/3dVB848rm4JVRpf91xwApd2aMjD4+fy5rTlUZcjSUihIUdt9sqt/Pof39K5SSYPXBb/kc3xkt0sNtXIF/PX8/D4I59qpLDQuev16Xw0Zy0PXHoiPTtmlWCV8ZWWZjzy0/ac0vIYfvOP6Xwxf13UJUmSUWjIUdn43R6uH5VDjUoZPHNNFyqUi2bcQUm5umsT+p/SjGFfLuK1KYf/m7a78+C7c3hj6gruOPdY+pzcrOSLjLMK5dJ5pk8XWtWtyo2jpzBzxZaoS5IkotCQI7a3oJCbX5zCuu27GdKnS9LcRnq07r3oBE5peQz/+8YMpi49vKlGnvgkj+H/WsSAU5vxi7OSb3xKWNUrZjDy2m5kVi7PgOcns2zjjqhLkiSh0JAj9uA7s/lm4UYevrwdHRrHf22MRMlIT+PJn3Wmfo3YVCOrt+wK9b7R3yzh/z78Nz/plMXvLmpTak/T7VOvekVGXtuVPfmF9Bs+iY3fafCfKDTkCI2ZtJSRXy/h+tObl/iiQcmgZpXYVCM7duczaPShB72N/XYl9701k3NOqMtfrmyfsAkI461V3WoM65fN8s07uW7kZHbu0eC/VKfQkMM2ZclGfhfh2hiJclz9ajx2dUemL9/C3a8ffKqRT+et5Y5XcunatBZP/KwzGell679V12a1GNyrI9OWbeYXLyffIlaSWJH86zazR8xsrplNN7M3zSyzyLZ7zCzPzOaZ2XlF2s8P2vLM7O4o6hZYtWUnN4yeSsPMSjzeuxPlytgPyP39+MT63HnusfwzdyVDDzB2YcqSjdz4whSOrVeNYf1jy8qWRee3bcAfLjmRj+as4b6xGvyXyqL6H/8h0Nbd2wP/Bu4BMLM2QC/gROB84CkzSzezdOBJ4AKgDdA72FcSaN/aGDv35PNs32wyK5ePuqSEuPWsVlzUrgEPvzeXCXPX/qd97uqtDBgxmfrVKzLy2m5UrxjtBITx1u+UZtzUoyUvTVzKkxPyoi5HIhJJaLj7B+6eH7z8Bth3UrwnMMbdd7v7IiAP6BY88tx9obvvAcYE+0qCuDv3vDGD6cu38NjVHTk2CdbGSBSz2NiFE+pX57aXp5G3djtLN8QmIKxcvhyjB55EnWoVoi4zIe467zgu75zFXz/4N//IWRZ1ORKBZDi3cC0wPnieBRT9l7g8aDtYuyTIsC8W8ea02NiDH59YP+pyEq5y+XI82y+b8uXSuH5UDtc8N5G9BYWMHtitVMyxVVLMjD9f0Z7TW9fm7jdmMGHe2kO/ScqUuIWGmX1kZjMP8OhZZJ97gXzgxRI87iAzyzGznHXrNJq1JHyzcAP/b/wcLmhbn1uTcG2MRMnKrMQzfbqwfNMO1m/fzYj+XZNiNcJEy0hP4+lrunB8/Wrc8uJUpi/fHHVJkkBxm7fa3c8pbruZ9QcuBs72/15VWwE0LrJbo6CNYtr3P+5QYChAdna2rtYdJXfn4fFzaVCjEn9N4FrWyaprs1q8MPAkqlQoR9usGlGXE5mqFcoxYkBXLn/qKwaMmMzYX5xGVmalqMuSwDvTV7J7byFXdCn52+GjunvqfOAu4FJ3LzrUdCzQy8wqmFlzoDUwCZgMtDaz5mZWntjF8rGJrjsVfZm3ntxlm7n5zJZUqZDca2MkykktjknpwNinbrXYDQDbdufz+Mfzoy5HApMWbeSOV77llcnL4rI2SlTXNJ4AqgEfmlmumT0D4O6zgFeB2cB7wC3uXhBcNL8VeB+YA7wa7Ctx5O78/aP5NKhRkSvj8BuLlH4t61SlV9fGvD51udYZTwIL1m3n+lE5NKpViaF9u5AehzMDUd091crdG7t7x+BxY5FtD7l7S3c/zt3HF2kf5+7HBtseiqLuVPP1wg3kLNnETT1alvqJCCV+bjijJe4w9LMFUZeS0tZt203/EZPISDee798tbrfEJ8PdU5KkBn88n7rVKnBVduND7ywpKyuzEld0bsSYyctYuy3cPF1Ssnbsyee6kZNZt203z/XrGtclhRUackATF27gm4UbufGMlmV2lLOUnJt6tGRvQSHPfbEo6lJSTkGhc9vLucxYsYXHe3eO++ShCg05oMc/yaN21Qr07tYk6lKkFGhWuwqXdmjI6G+WsEmz4SaMu3P/27P4aM4a/nDpiZzbpl7cj6nQkB+YsmQjX+at54YftaBSefUyJJxbzmzFjj0FjPiXehuJMuyLRYwKZpvum6AFvxQa8gODP86jVpXy/Ly7ehkSXut61bigbX1GfLWYrbv2Rl1Omffu9FU8NG4OF7arzz0XnJCw4yo05Htyl23ms3+v4/rTW1C5vMZlyOG55cxWbNuVz+ivl0RdSpmWs3gjv3o1l+ymNXn0qo4JHXSr0JDvefzj+WRWzqDPyU2jLkVKobZZNTjr+LoM+2IhO/bkH/oNctgWrtvOdaNyyMqsxLN9Ez8dv0JD/mPmii18PHct153WnKoa/S1H6JYzW7Fpx15emrg06lLKnPXbd9N/xGTSzXh+QFdqVkn88gQKDfmPwR/Pp3rFcvQ9pVnUpUgp1qVpTU5tdQxDPl94yGVyJbydewq4bmQOa7ftYli/bJoeUyWSOhQaAsDslVv5YPYarj2teZlfTEji79YzW7Nu226tuVFCCgqd28dM49vlm/l7r050alIzsloUGgLAExPmU61COQac0jzqUqQM6N6iFtlNa/LMZwvZk681xY/WH9+ZzQez13DfxW04L+L1bBQawrzV2xg3YzX9T21GjcrqZcjRMzNuPasVKzbv5J/TDriKgYT03JeLeP6rxQw8rTkDTo3+lzqFhvDEhDyqlE/n2iT4ByllxxnH1qFdVg2e+jSP/AL1No7E+BmrePDd2VzQtj73Xpi4sRjFUWikuLy123ln+kr6ntIskjsxpOza19tYvGEH785YFXU5pc6UJRv55Su5dGqcyWNXJ3YsRnEUGinuyQl5VCyXznWnqZchJe/cE+pxXL1qPPFJHoVxWBCorFq0/juuG5lDgxoVGdava1JNGqrQSGGL1n/HW7kr6HNyU46pWiHqcqQMSkszbjmrFfPXbueD2aujLqdU2LB9NwNGTMLMeH5AN2ol2RkAhUYKe3JCHhnpaVx/eouoS5Ey7KJ2DWheuwqPf5KHu3obxdm1t4DrRuWwassunu2bTbPa0YzFKI5CI0Ut3bCDN6et4OcnNaVONfUyJH7S04ybe7Rk1sqtfDpvXdTlJK2CQueXY3LJXbaZv/fqSJem0Y3FKI5CI0U99Wke6WnGDWeolyHxd1mnLLIyKzH4k/nqbRzEn8bN4b1Zq/ntRW04v22DqMs5KIVGClq+aQevTVlO766NqVe9YtTlSArISE/jph4tmega3CoAAAjLSURBVLZ0M18v2BB1OUln+JeLeO7LRQw4tRkDk/ymFIVGCnr60wWkmXFjj5ZRlyIp5MoujahXvQKPf5IXdSlJ5b2Zq/nju7M578R6/PaiNlGXc0gKjRSzcvNOXs1Zxk+zG9GgRqWoy5EUUjEjnUE/asnXCzeQs3hj1OUkhalLN3H7mGl0aJTJ367uRHqSjMUojkIjxQz5bAHucJN6GRKB3t0ac0yV8jwxQb2NJRtiYzHq16jIc/2yS83SygqNFLJm6y5enryMK7s0olHNylGXIymocvlyDDy9OZ/OW8eM5VuiLicyG7/bQ/8Rk3F3RvTvWqrGSSk0UsiQzxZSUOjc3KNV1KVICuvTvSnVK5bjiQnzoy4lErv2FnD9qBxWbN7JsH7ZtKhTNeqSDotCI0Ws3baLFycu4SedsmhyjHoZEp1qFTMYcGpz3p+1hnmrt0VdTkIVFjp3vJrL1KWb+NvVHenStFbUJR02hUaKGPbFIvYWFHLLmeplSPQGnNqMKuXTeTLFrm38v/FzGDdjNfdeeAIXtkvesRjFUWikgA3bdzP66yX07JhF8ySclkBST2bl8vQ5uRnvTF/JwnXboy4nIUZ+tZhnv1hEv5ObJv1YjOJEEhpm9kczm25muWb2gZk1DNrNzAabWV6wvXOR9/Qzs/nBo18UdZdWw75cxK78AvUyJKlcd3pzypdL4+lPF0RdStx9MGs19789i3NOqMd9l5yIWfLfWnswUfU0HnH39u7eEXgHuC9ovwBoHTwGAU8DmFkt4PfASUA34PdmlpwTsySZTd/tYdRXi7m4fUNa1S1dF9ykbKtdtQK9uzXhzWkrWLZxR9TlxE3uss3cNmYa7Rpl8njv0jEWoziRhIa7by3ysgqwbzKansAoj/kGyDSzBsB5wIfuvtHdNwEfAucntOhSavi/FvHdngJ+cZZ6GZJ8Bv2oBWlmDPm8bPY2lm7YwcDnJ1OnWoVSNRajOOWiOrCZPQT0BbYAZwbNWcCyIrstD9oO1n6gzx1ErJcCsN3M5h1FmbWB9Ufx/qRx3J+P+iPKzHdRQvR9fN9RfR8PBY8y4oDfRZ3/iaCSI9f0YBviFhpm9hFQ/wCb7nX3t9z9XuBeM7sHuJXY6aej5u5DgaEl8VlmluPu2SXxWaWdvovv0/fxffo+/qusfxdxCw13Pyfkri8C44iFxgqgcZFtjYK2FUCP/do/PeoiRUTksER191TrIi97AnOD52OBvsFdVN2BLe6+Cngf+LGZ1QwugP84aBMRkQSK6prGw2Z2HFAILAFuDNrHARcCecAOYACAu280sz8Ck4P9HnD3REyTWSKnucoIfRffp+/j+/R9/FeZ/i5Mq2iJiEhYGhEuIiKhKTRERCQ0hcYBmNn5ZjYvmM7k7qjriZKZNTazCWY228xmmdntUdcUNTNLN7NpZvZO1LVEzcwyzew1M5trZnPM7OSoa4qSmf0q+H8y08xeNrOKUddU0hQa+zGzdOBJYlOatAF6m1nyL9wbP/nAne7eBugO3JLi3wfA7cCcqItIEn8H3nP344EOpPD3YmZZwG1Atru3BdKBXtFWVfIUGj/UDchz94XuvgcYQ+y24JTk7qvcfWrwfBuxHwoHHI2fCsysEXARMCzqWqJmZjWAHwHPAbj7HnffHG1VkSsHVDKzckBlYGXE9ZQ4hcYPhZ6yJNWYWTOgEzAx2koi9TfgLmK3i6e65sA6YERwum6YmaXs3PvuvgL4K7AUWEVsnNkH0VZV8hQaEoqZVQVeB36534STKcPMLgbWuvuUqGtJEuWAzsDT7t4J+A5I2WuAwcDjnsTCtCFQxcyuibaqkqfQ+KGDTWWSsswsg1hgvOjub0RdT4ROBS41s8XETlueZWYvRFtSpJYDy919X8/zNWIhkqrOARa5+zp33wu8AZwScU0lTqHxQ5OB1mbW3MzKE7uQNTbimiJjsdVingPmuPujUdcTJXe/x90buXszYv8uPnH3MvebZFjuvhpYFszuAHA2MDvCkqK2FOhuZpWD/zdnUwZvDIhsavRk5e75ZnYrsbmt0oHh7j4r4rKidCrQB5hhZrlB2/+6+7gIa5Lk8QvgxeAXrIUEU/+kInefaGavAVOJ3XU4jTI4pYimERERkdB0ekpEREJTaIiISGgKDRERCU2hISIioSk0REQkNIWGyH7M7Bgzyw0eq81sRfB8u5k9Fcfj9jCzMjcYTMoWjdMQ2Y+7bwA6ApjZH4Dt7v7XBBy6B7Ad+CoBxxI5IuppiIQU9ATeCZ7/wcxGmtkXZrbEzC43s7+Y2Qwzey+YegUz62Jmn5nZFDN738waBO23BWuUTDezMcFkkDcCvwp6NaebWR0ze93MJgePU4sce7SZfW1m883s+qC9gZl9Hrx/ppmdHsX3JGWbehoiR64lcCaxdVe+Bq5w97vM7E3gIjN7F3gc6Onu68zsauAh4FpiE/s1d/fdZpbp7pvN7BmK9GrM7CXgMXf/0syaEJul4ITg2O2JrW9SBZgWHKs38L67PxSsC1M5MV+DpBKFhsiRG+/ue81sBrEpZ94L2mcAzYDjgLbAh7GpiEgnNmU2wHRi02/8E/jnQT7/HKBN8F6A6sFswwBvuftOYKeZTSC2DsxkYHjQy/mnu+f+4BNFjpJCQ+TI7QZw90Iz2+v/nZOnkNj/LQNmufuBlkC9iNgCRpcA95pZuwPskwZ0d/ddRRuDENl//h9398/N7EfBZz9vZo+6+6gj/LuJHJCuaYjEzzygzr51s80sw8xONLM0oLG7TwD+B6gBVAW2AdWKvP8DYhMCEry/Y5FtPc2sopkdQ+wC+mQzawqscfdnia0smMrTlEucKDRE4iRYLvhK4M9m9i2QS2x9hXTgheC01jRgcLBM6tvAT/ZdCCdYbzq4WD6b2IXyfaYDE4BvgD+6+0pi4fGtmU0Dria2frdIidIstyKlTIJvAxb5HvU0REQkNPU0REQkNPU0REQkNIWGiIiEptAQEZHQFBoiIhKaQkNEREL7/wf1/quUfts6AAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "x, y = ts2xy(load_results(log_dir), 'episodes')  # Organising the logged results in to a clean format for plotting. timesteps\n",
        "plt.plot(x,y)\n",
        "plt.ylim([-300, 300])\n",
        "plt.xlabel('Timesteps')\n",
        "plt.ylabel('Episode Rewards')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "mw3iMGmwobbo"
      },
      "source": [
        "From the above plot, we observe that, although the maximum reward is achieved quickly. Achieving an episodic reward of > 200 is good. We see that the agent has achieved it in less than 50000 timesteps (speed is good!). However, there are a lot of fluctuations in the performance (stability is not good!). \n",
        "\n",
        "Your objective now is to modify the model parameters (nn_layers, learning_rate in the code cell #2 above), run all the cells following it and investigate the stability and speed of the chosen configuration.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "_bKzapIyobbp"
      },
      "source": [
        "---\n",
        "# Additional Project Ideas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "gzByWGxfobbq"
      },
      "source": [
        "## 1 Play with exploration-exploitation trade-off\n",
        "\n",
        "Exploration (selecting random actions) and exploitation (selecting greedy action) is a crucial component of the DQN algorithm. Explore random actions for a long time will slow down the training process. At the same time, if all actions are not explored enough, it might lead to a sub-optimal performance. In the DQN code above, we have used the following parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "5TeOBG-Jobbq"
      },
      "outputs": [],
      "source": [
        "exploration_initial_eps = 1  # initial value of random action probability. Range is between 0 and 1.\n",
        "exploration_fraction = 0.5  # fraction of entire training period over which the exploration rate is reduced. Range is between 0 and 1.\n",
        "exploration_final_eps = 0.05  # (set by defualt) final value of random action probability. Range is between 0 and 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "dZuGeAULobbq"
      },
      "source": [
        "Your objective is to play around with these parameters and analyze their performance (speed and stability). You can modify these parameters and set them as arguments in DQN(...,exploration_initial_eps = 1, exploration_fraction = 0.5, exploration_final_eps = 0.05,...). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "yGp5JFoAobbr"
      },
      "source": [
        "## 2 Reward Shaping\n",
        "\n",
        "Your objective here is to construct a modified reward function that improves the performance of the Lunar Lander. To this end, you would have to create your own custom environment. An example of a custom environment is given below:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ref: https://towardsdatascience.com/beginners-guide-to-custom-environments-in-openai-s-gym-989371673952"
      ],
      "metadata": {
        "id": "yIFSDq7eyXY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### New environment with wind"
      ],
      "metadata": {
        "id": "iIvGmuJD49zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.envs.box2d import LunarLander\n",
        "from typing import TYPE_CHECKING, Optional\n",
        "from gym.utils import EzPickle, colorize\n",
        "try:\n",
        "    import Box2D\n",
        "    from Box2D.b2 import (\n",
        "        circleShape,\n",
        "        contactListener,\n",
        "        edgeShape,\n",
        "        fixtureDef,\n",
        "        polygonShape,\n",
        "        revoluteJointDef,\n",
        "    )\n",
        "except ImportError:\n",
        "    raise DependencyNotInstalled(\"box2d is not installed, run `pip install gym[box2d]`\")\n",
        "import math"
      ],
      "metadata": {
        "id": "dTxYseql3ry3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "6758290e-3e00-4c5b-a3e8-46ba413a4fae"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-97fe1a8faaf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mDependencyNotInstalled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"box2d is not installed, run `pip install gym[box2d]`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym.utils.renderer'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FPS = 50\n",
        "SCALE = 30.0  # affects how fast-paced the game is, forces should be adjusted as well\n",
        "\n",
        "MAIN_ENGINE_POWER = 13.0\n",
        "SIDE_ENGINE_POWER = 0.6\n",
        "\n",
        "INITIAL_RANDOM = 1000.0  # Set 1500 to make game harder\n",
        "\n",
        "LANDER_POLY = [(-14, +17), (-17, 0), (-17, -10), (+17, -10), (+17, 0), (+14, +17)]\n",
        "LEG_AWAY = 20\n",
        "LEG_DOWN = 18\n",
        "LEG_W, LEG_H = 2, 8\n",
        "LEG_SPRING_TORQUE = 40\n",
        "\n",
        "SIDE_ENGINE_HEIGHT = 14.0\n",
        "SIDE_ENGINE_AWAY = 12.0\n",
        "\n",
        "VIEWPORT_W = 600\n",
        "VIEWPORT_H = 400"
      ],
      "metadata": {
        "id": "E6Ivff2jW7q8"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Custom_LunarLander_wind(LunarLander):\n",
        "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        render_mode: Optional[str] = None,\n",
        "        continuous: bool = False,\n",
        "        gravity: float = -10.0,\n",
        "        enable_wind: bool = False,\n",
        "        wind_power: float = 15.0,\n",
        "        turbulence_power: float = 1.5,\n",
        "    ):\n",
        "        # EzPickle.__init__(self)\n",
        "\n",
        "        assert (\n",
        "            -12.0 < gravity and gravity < 0.0\n",
        "        ), f\"gravity (current value: {gravity}) must be between -12 and 0\"\n",
        "        self.gravity = gravity\n",
        "\n",
        "        if 0.0 > wind_power or wind_power > 20.0:\n",
        "            warnings.warn(\n",
        "                colorize(\n",
        "                    f\"WARN: wind_power value is recommended to be between 0.0 and 20.0, (current value: {wind_power})\",\n",
        "                    \"yellow\",\n",
        "                ),\n",
        "            )\n",
        "        self.wind_power = wind_power\n",
        "\n",
        "        if 0.0 > turbulence_power or turbulence_power > 2.0:\n",
        "            warnings.warn(\n",
        "                colorize(\n",
        "                    f\"WARN: turbulence_power value is recommended to be between 0.0 and 2.0, (current value: {turbulence_power})\",\n",
        "                    \"yellow\",\n",
        "                ),\n",
        "            )\n",
        "        self.turbulence_power = turbulence_power\n",
        "\n",
        "        self.enable_wind = enable_wind\n",
        "        # self.wind_idx = np.random.randint(-9999, 9999)\n",
        "        # self.torque_idx = np.random.randint(-9999, 9999)\n",
        "\n",
        "        # self.screen: pygame.Surface = None\n",
        "        # self.clock = None\n",
        "        # self.isopen = True\n",
        "        # self.world = Box2D.b2World(gravity=(0, gravity))\n",
        "        # self.moon = None\n",
        "        # self.lander: Optional[Box2D.b2Body] = None\n",
        "        # self.particles = []\n",
        "\n",
        "        # self.prev_reward = None\n",
        "\n",
        "        self.continuous = continuous\n",
        "\n",
        "        # low = np.array(\n",
        "        #     [\n",
        "        #         # these are bounds for position\n",
        "        #         # realistically the environment should have ended\n",
        "        #         # long before we reach more than 50% outside\n",
        "        #         -1.5,\n",
        "        #         -1.5,\n",
        "        #         # velocity bounds is 5x rated speed\n",
        "        #         -5.0,\n",
        "        #         -5.0,\n",
        "        #         -math.pi,\n",
        "        #         -5.0,\n",
        "        #         -0.0,\n",
        "        #         -0.0,\n",
        "        #     ]\n",
        "        # ).astype(np.float32)\n",
        "        # high = np.array(\n",
        "        #     [\n",
        "        #         # these are bounds for position\n",
        "        #         # realistically the environment should have ended\n",
        "        #         # long before we reach more than 50% outside\n",
        "        #         1.5,\n",
        "        #         1.5,\n",
        "        #         # velocity bounds is 5x rated speed\n",
        "        #         5.0,\n",
        "        #         5.0,\n",
        "        #         math.pi,\n",
        "        #         5.0,\n",
        "        #         1.0,\n",
        "        #         1.0,\n",
        "        #     ]\n",
        "        # ).astype(np.float32)\n",
        "\n",
        "        # # useful range is -1 .. +1, but spikes can be higher\n",
        "        # self.observation_space = spaces.Box(low, high)\n",
        "\n",
        "        # if self.continuous:\n",
        "        #     # Action is two floats [main engine, left-right engines].\n",
        "        #     # Main engine: -1..0 off, 0..+1 throttle from 50% to 100% power. Engine can't work with less than 50% power.\n",
        "        #     # Left-right:  -1.0..-0.5 fire left engine, +0.5..+1.0 fire right engine, -0.5..0.5 off\n",
        "        #     self.action_space = spaces.Box(-1, +1, (2,), dtype=np.float32)\n",
        "        # else:\n",
        "        #     # Nop, fire left engine, main engine, right engine\n",
        "        #     self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "        # self.renderer = Renderer(self.render_mode, self._render)\n",
        "        LunarLander.__init__(self, render_mode)\n",
        "\n",
        "    def step(self, action):\n",
        "        assert self.lander is not None\n",
        "\n",
        "        # Update wind\n",
        "        assert self.lander is not None, \"You forgot to call reset()\"\n",
        "        if self.enable_wind and not (\n",
        "            self.legs[0].ground_contact or self.legs[1].ground_contact\n",
        "        ):\n",
        "            # the function used for wind is tanh(sin(2 k x) + sin(pi k x)),\n",
        "            # which is proven to never be periodic, k = 0.01\n",
        "            wind_mag = (\n",
        "                math.tanh(\n",
        "                    math.sin(0.02 * self.wind_idx)\n",
        "                    + (math.sin(math.pi * 0.01 * self.wind_idx))\n",
        "                )\n",
        "                * self.wind_power\n",
        "            )\n",
        "            self.wind_idx += 1\n",
        "            self.lander.ApplyForceToCenter(\n",
        "                (wind_mag, 0.0),\n",
        "                True,\n",
        "            )\n",
        "\n",
        "            # the function used for torque is tanh(sin(2 k x) + sin(pi k x)),\n",
        "            # which is proven to never be periodic, k = 0.01\n",
        "            torque_mag = math.tanh(\n",
        "                math.sin(0.02 * self.torque_idx)\n",
        "                + (math.sin(math.pi * 0.01 * self.torque_idx))\n",
        "            ) * (self.turbulence_power)\n",
        "            self.torque_idx += 1\n",
        "            self.lander.ApplyTorque(\n",
        "                (torque_mag),\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        if self.continuous:\n",
        "            action = np.clip(action, -1, +1).astype(np.float32)\n",
        "        else:\n",
        "            assert self.action_space.contains(\n",
        "                action\n",
        "            ), f\"{action!r} ({type(action)}) invalid \"\n",
        "\n",
        "        # Engines\n",
        "        tip = (math.sin(self.lander.angle), math.cos(self.lander.angle))\n",
        "        side = (-tip[1], tip[0])\n",
        "        dispersion = [self.np_random.uniform(-1.0, +1.0) / SCALE for _ in range(2)]\n",
        "\n",
        "        m_power = 0.0\n",
        "        if (self.continuous and action[0] > 0.0) or (\n",
        "            not self.continuous and action == 2\n",
        "        ):\n",
        "            # Main engine\n",
        "            if self.continuous:\n",
        "                m_power = (np.clip(action[0], 0.0, 1.0) + 1.0) * 0.5  # 0.5..1.0\n",
        "                assert m_power >= 0.5 and m_power <= 1.0\n",
        "            else:\n",
        "                m_power = 1.0\n",
        "            # 4 is move a bit downwards, +-2 for randomness\n",
        "            ox = tip[0] * (4 / SCALE + 2 * dispersion[0]) + side[0] * dispersion[1]\n",
        "            oy = -tip[1] * (4 / SCALE + 2 * dispersion[0]) - side[1] * dispersion[1]\n",
        "            impulse_pos = (self.lander.position[0] + ox, self.lander.position[1] + oy)\n",
        "            p = self._create_particle(\n",
        "                3.5,  # 3.5 is here to make particle speed adequate\n",
        "                impulse_pos[0],\n",
        "                impulse_pos[1],\n",
        "                m_power,\n",
        "            )  # particles are just a decoration\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * MAIN_ENGINE_POWER * m_power, oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * MAIN_ENGINE_POWER * m_power, -oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        s_power = 0.0\n",
        "        if (self.continuous and np.abs(action[1]) > 0.5) or (\n",
        "            not self.continuous and action in [1, 3]\n",
        "        ):\n",
        "            # Orientation engines\n",
        "            if self.continuous:\n",
        "                direction = np.sign(action[1])\n",
        "                s_power = np.clip(np.abs(action[1]), 0.5, 1.0)\n",
        "                assert s_power >= 0.5 and s_power <= 1.0\n",
        "            else:\n",
        "                direction = action - 2\n",
        "                s_power = 1.0\n",
        "            ox = tip[0] * dispersion[0] + side[0] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            oy = -tip[1] * dispersion[0] - side[1] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            impulse_pos = (\n",
        "                self.lander.position[0] + ox - tip[0] * 17 / SCALE,\n",
        "                self.lander.position[1] + oy + tip[1] * SIDE_ENGINE_HEIGHT / SCALE,\n",
        "            )\n",
        "            p = self._create_particle(0.7, impulse_pos[0], impulse_pos[1], s_power)\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * SIDE_ENGINE_POWER * s_power, oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)\n",
        "\n",
        "        pos = self.lander.position\n",
        "        vel = self.lander.linearVelocity\n",
        "        state = [\n",
        "            (pos.x - VIEWPORT_W / SCALE / 2) / (VIEWPORT_W / SCALE / 2),\n",
        "            (pos.y - (self.helipad_y + LEG_DOWN / SCALE)) / (VIEWPORT_H / SCALE / 2),\n",
        "            vel.x * (VIEWPORT_W / SCALE / 2) / FPS,\n",
        "            vel.y * (VIEWPORT_H / SCALE / 2) / FPS,\n",
        "            self.lander.angle,\n",
        "            20.0 * self.lander.angularVelocity / FPS,\n",
        "            1.0 if self.legs[0].ground_contact else 0.0,\n",
        "            1.0 if self.legs[1].ground_contact else 0.0,\n",
        "        ]\n",
        "        assert len(state) == 8\n",
        "\n",
        "        reward = 0\n",
        "        shaping = (\n",
        "            -100 * np.sqrt(state[0] * state[0] + state[1] * state[1])\n",
        "            - 100 * np.sqrt(state[2] * state[2] + state[3] * state[3])\n",
        "            - 100 * abs(state[4])\n",
        "            + 10 * state[6]\n",
        "            + 10 * state[7]\n",
        "        )  # And ten points for legs contact, the idea is if you\n",
        "        # lose contact again after landing, you get negative reward\n",
        "        if self.prev_shaping is not None:\n",
        "            reward = shaping - self.prev_shaping\n",
        "        self.prev_shaping = shaping\n",
        "\n",
        "        reward -= (\n",
        "            m_power * 0.30\n",
        "        )  # less fuel spent is better, about -30 for heuristic landing\n",
        "        reward -= s_power * 0.03\n",
        "\n",
        "        terminated = False\n",
        "        if self.game_over or abs(state[0]) >= 1.0:\n",
        "            terminated = True\n",
        "            reward = -100\n",
        "        if not self.lander.awake:\n",
        "            terminated = True\n",
        "            reward = +100\n",
        "        self.renderer.render_step()\n",
        "        return np.array(state, dtype=np.float32), reward, terminated, False, {}\n",
        "\n",
        "\n",
        "    \n",
        "  # def reset(self):\n",
        "  #     pass  # reward, done, info can't be included\n",
        "\n",
        "  # def render(self, mode='human'):\n",
        "  #     pass\n",
        "\n",
        "  # def close(self):\n",
        "  #     pass"
      ],
      "metadata": {
        "id": "7Nxu5rsZ4MH1"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Enter the name of the custome environment you created and uncomment the line below.\n",
        "new_env = Custom_LunarLander_wind()\n",
        "\n",
        "new_env = stable_baselines3.common.monitor.Monitor(new_env, log_dir)\n",
        "# Refer to https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html, if you would like to create more complex environments."
      ],
      "metadata": {
        "id": "hcHdliZ_4kSl"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_env = Custom_LunarLander_wind()\n",
        "new_env"
      ],
      "metadata": {
        "id": "GdKLT2SiGCiV",
        "outputId": "407598a3-fbaa-4abb-e569-7e49af691a6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Custom_LunarLander_wind at 0x7f1a0797b650>"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "check_env(new_env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "kG6IM6zW2IPm",
        "outputId": "b254e71d-0994-418f-810f-e87a9de3093b"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-142-25e5f3051d27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_checker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcheck_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/env_checker.py\u001b[0m in \u001b[0;36mcheck_env\u001b[0;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# ============ Check the returned values ===============\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m     \u001b[0m_check_returned_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;31m# ==== Check the render method and the declared render modes ====\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/env_checker.py\u001b[0m in \u001b[0;36m_check_returned_values\u001b[0;34m(env, observation_space, action_space)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \"\"\"\n\u001b[1;32m    141\u001b[0m     \u001b[0;31m# because env inherits from gym.Env, we assume that `reset()` and `step()` methods exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrawlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinuous\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_particle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mttl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-139-7d9ed2c5f49a>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mterminated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Custom_LunarLander_wind' object has no attribute 'renderer'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_env = wrap_env(new_env)\n",
        "new_env"
      ],
      "metadata": {
        "id": "8kRVje8cGXCw",
        "outputId": "23e94417-3e01-42d5-8475-2cca464bf242",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/monitor.py:87: UserWarning: \u001b[33mWARN: Trying to monitor an environment which has no 'spec' set. This usually means you did not create it via 'gym.make', and is recommended only for advanced users.\u001b[0m\n",
            "  \"Trying to monitor an environment which has no 'spec' set. This usually means you did not create it via 'gym.make', and is recommended only for advanced users.\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Monitor<Monitor<Custom_LunarLander_wind instance>>>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.envs.registration import register\n",
        "# Example for the CartPole environment\n",
        "register(\n",
        "    # unique identifier for the env `name-version`\n",
        "    id=\"LunarLander-v3\",\n",
        "    # path to the class for creating the env\n",
        "    # Note: entry_point also accept a class as input (and not only a string)\n",
        "    entry_point= Custom_LunarLander_wind,\n",
        "    # Max number of steps per episode, using a `TimeLimitWrapper`\n",
        "    max_episode_steps=500,\n",
        ")"
      ],
      "metadata": {
        "id": "C1cIXPTaJsQG",
        "outputId": "ee0d6363-59aa-433c-cb71-afa52d976add",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:216: UserWarning: \u001b[33mWARN: Overriding environment LunarLander-v3\u001b[0m\n",
            "  logger.warn(\"Overriding environment {}\".format(id))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_env = wrap_env(gym.make(\"LunarLander-v3\"))\n",
        "\n",
        "observation = new_env.reset()\n",
        "total_reward = 0\n",
        "\n",
        "while True:\n",
        "  new_env.render()\n",
        "  action, states = model.predict(observation, deterministic=True)\n",
        "  observation, reward, done, info = new_env.step(action)\n",
        "  total_reward += reward\n",
        "  if done:\n",
        "    break;\n",
        "\n",
        "# print(total_reward)\n",
        "new_env.close()\n",
        "show_video()"
      ],
      "metadata": {
        "id": "HeHfAR-g4ttm",
        "outputId": "3c9198ae-b9b1-444a-fad6-54343ec69447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-5381423c68d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnew_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LunarLander-v3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrawlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinuous\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_particle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mttl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-30bde1432ea5>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Update wind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You forgot to call reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         if self.enable_wind and not (\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mground_contact\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mground_contact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         ):\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Custom_LunarLander_wind' object has no attribute 'enable_wind'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_env"
      ],
      "metadata": {
        "id": "t6WGxfuLKcm3",
        "outputId": "e25a0e88-494c-4b53-864b-0429879c5dfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Monitor<TimeLimit<Custom_LunarLander_wind<LunarLander-v3>>>>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Template"
      ],
      "metadata": {
        "id": "h7Jy40kF4cwx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {},
        "id": "RStcMxLfobbr"
      },
      "outputs": [],
      "source": [
        "# Taken from https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html\n",
        "class CustomEnv(gym.Env):\n",
        "  \"\"\"Custom Environment that follows gym interface\"\"\"\n",
        "  metadata = {'render.modes': ['human']}\n",
        "\n",
        "  def __init__(self, arg1, arg2):\n",
        "    super(CustomEnv, self).__init__()\n",
        "    # Define action and observation space\n",
        "    # They must be gym.spaces objects\n",
        "    # Example when using discrete actions:\n",
        "    self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n",
        "    # Example for using image as input (channel-first; channel-last also works):\n",
        "    self.observation_space = spaces.Box(low=0, high=255,\n",
        "                                        shape=(N_CHANNELS, HEIGHT, WIDTH), dtype=np.uint8)\n",
        "\n",
        "  def step(self, action):\n",
        "    ...\n",
        "    return observation, reward, done, info\n",
        "  def reset(self):\n",
        "    ...\n",
        "    return observation  # reward, done, info can't be included\n",
        "  def render(self, mode='human'):\n",
        "    ...\n",
        "  def close (self):\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "RKAA_XL4obbs"
      },
      "source": [
        "As you are only changing the reward structure, you can inherit the original Lunar Lander environment from https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py and modify just the \"step\" function. Focus on modifying the following part of the code in the \"step\" function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "vW-1yrPyobbs"
      },
      "outputs": [],
      "source": [
        "def step(self, actions):\n",
        "  ...\n",
        "  ...\n",
        "  ...\n",
        "  reward = 0\n",
        "  shaping = (\n",
        "      -100 * np.sqrt(state[0] * state[0] + state[1] * state[1])\n",
        "      - 100 * np.sqrt(state[2] * state[2] + state[3] * state[3])\n",
        "      - 100 * abs(state[4])\n",
        "      + 10 * state[6]\n",
        "      + 10 * state[7]\n",
        "  )  # And ten points for legs contact, the idea is if you\n",
        "  # lose contact again after landing, you get negative reward\n",
        "  if self.prev_shaping is not None:\n",
        "      reward = shaping - self.prev_shaping\n",
        "  self.prev_shaping = shaping\n",
        "\n",
        "  reward -= (\n",
        "      m_power * 0.30\n",
        "  )  # less fuel spent is better, about -30 for heuristic landing. You should modify these values.\n",
        "  reward -= s_power * 0.03\n",
        "\n",
        "  done = False\n",
        "  if self.game_over or abs(state[0]) >= 1.0:\n",
        "      done = True\n",
        "      reward = -100\n",
        "  if not self.lander.awake:\n",
        "      done = True\n",
        "      reward = +100\n",
        "  return np.array(state, dtype=np.float32), reward, done, {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "9CCbUKXQobbs"
      },
      "source": [
        "Once you have cutomized your own environment, you can execute that environment by just calling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "T7IuLBYxobbt"
      },
      "outputs": [],
      "source": [
        "#Enter the name of the custome environment you created and uncomment the line below.\n",
        "#env = Custom_LunarLander()\n",
        "# Refer to https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html, if you would like to create more complex environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ZeEbxPsFobbt"
      },
      "source": [
        "## 3 Identify the state information crucial to its performance.\n",
        "\n",
        "Your objective here is to alter the input state information and analyze the performance. The input state of the Lunar Lander consists of following components:\n",
        "\n",
        "  1. Horizontal Position\n",
        "  2. Vertical Position\n",
        "  3. Horizontal Velocity\n",
        "  4. Vertical Velocity\n",
        "  5. Angle\n",
        "  6. Angular Velocity\n",
        "  7. Left Leg Contact\n",
        "  8. Right Leg Contact\n",
        "\n",
        "You can train the algorithm by masking one of the eight components at a time and understand how that affects the performance of the algorithm. Similar to the reward shaping task, you would have to create a custom environment and modify the state space. Again, you can inherit all the necessary functions and modify the following portion of the \"Step\" function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "e0qQ--Tfobbt"
      },
      "outputs": [],
      "source": [
        "def step(self, actions):\n",
        "  ...\n",
        "  ...\n",
        "  ...\n",
        "  state = [ # Remove one component at a time to investigate the effect on performance!\n",
        "            (pos.x - VIEWPORT_W / SCALE / 2) / (VIEWPORT_W / SCALE / 2),\n",
        "            (pos.y - (self.helipad_y + LEG_DOWN / SCALE)) / (VIEWPORT_H / SCALE / 2),\n",
        "            vel.x * (VIEWPORT_W / SCALE / 2) / FPS,\n",
        "            vel.y * (VIEWPORT_H / SCALE / 2) / FPS,\n",
        "            self.lander.angle,\n",
        "            20.0 * self.lander.angularVelocity / FPS,\n",
        "            1.0 if self.legs[0].ground_contact else 0.0,\n",
        "            1.0 if self.legs[1].ground_contact else 0.0,\n",
        "        ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "EIifpb9Kobbu"
      },
      "source": [
        "## 4 Extension to Atari Games\n",
        "\n",
        "In the Lunar Lander task, the input to the algorithm is a vector of state information. Deep RL algorithms can also be applied when the input to the training is image frames, which is the case in the Atari games. For example, consider an Atari game - Pong. In this environment, the observation is an RGB image of the screen, which is an array of shape (210, 160, 3). To train the Pong game, you can start with the following sample code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "9GEIYsVVobbv"
      },
      "outputs": [],
      "source": [
        "## Taken from: https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/atari_games.ipynb#scrollTo=f3K4rMXwimBO\n",
        "env = make_atari_env('PongNoFrameskip-v4', n_envs=4, seed=0)\n",
        "\n",
        "##Atari Games take a lot of memory. Following commands crash on Coalb. Run the following code on Colab Pro or your local Jupyter notebook!\n",
        "# env = VecFrameStack(env, n_stack=4)\n",
        "# model = DQN('CnnPolicy', env, verbose=1)  # Note the difference here! We use 'CnnPolicy\" here instead of 'MlpPolicy' as the input is frames.\n",
        "# model.learn(total_timesteps=1) #change the number of timesteps as desired and run this command!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "5I26CZd4obbv"
      },
      "source": [
        "## 5 Obstacle Avoidance and Transfer Learning\n",
        "\n",
        "Your obstacle here is to add an obstacle in the path of the lunar lander (by creating a custom environment as described in point 2 above) and train the model such that the lander lands safely, avoiding collisions. \n",
        "\n",
        "You would first want to devise a mechansim for adding obstacles. For example, you could have an imaginary obstacle at some horizantal and vertical position cooridnates and modify the reward function such that a penalty is levied if the lander comes close to it. \n",
        "\n",
        "An interesting approach to solve this problem is to apply the techniques of transfer learning. For example, you could initialise the neural network model with the weights of the trained model on the original problem to improve the sample effeciency. This can be done using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "lSyhxVoLobbw"
      },
      "outputs": [],
      "source": [
        "## Specify the load path and uncomment below:\n",
        "\n",
        "# model = load(load_path,\n",
        "#              env=gym.make('LunarLander-v2'),\n",
        "#              custom_objects=None, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "4Qy8qSysobbw"
      },
      "source": [
        "Following are some of the resources on transfer learning that you would want to start with. \n",
        "\n",
        "**Research Papers**\n",
        "\n",
        "Surveys:\n",
        "1. (Long, Old, Highly cited) Taylor, M. E.,  et al. (2009). Transfer learning for reinforcement learning domains. https://www.jmlr.org/papers/volume10/taylor09a/taylor09a.pdf\n",
        "\n",
        "2. (Medium, Old, Good for a quick read) Lazaric, A. (2012). Transfer in reinforcement learning: a framework and a survey. https://hal.inria.fr/docs/00/77/26/26/PDF/transfer.pdf\n",
        "\n",
        "3. (Medium, Recent, Good for a quick read) Zhu, Z., Lin, K., & Zhou, J. (2020). Transfer learning in deep reinforcement learning. https://arxiv.org/pdf/2009.07888.pdf\n",
        "\n",
        "4. Specific example:\n",
        "Barreto, A., et al. (2016).  Successor features for transfer in reinforcement learning. https://arxiv.org/pdf/1606.05312"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "vJvZzNrAobbx"
      },
      "source": [
        "## 5(b) Transfer Learning in minigrid environment\n",
        "\n",
        "These are some simple gridworld gym environments designed to be particularly simple, lightweight and fast. Refer to https://github.com/maximecb/gym-minigrid for description of the environments. An example to load a minigrid environment is given below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "88Bk-iqzobby"
      },
      "outputs": [],
      "source": [
        "!pip install gym-minigrid --quiet\n",
        "import gym_minigrid\n",
        "env = gym.make('MiniGrid-Empty-5x5-v0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "lOL5WYR3obby"
      },
      "source": [
        "You can train a standard DQN agent in this env by wrapping the env with full image observation wrappers:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "PX71Rz3Uobbz"
      },
      "outputs": [],
      "source": [
        "env = gym_minigrid.wrappers.ImgObsWrapper(gym_minigrid.wrappers.RGBImgObsWrapper(env))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "o2UiCI5Iobbz"
      },
      "source": [
        "Note that with full image observations, the shape of the image observations may differ between envs. For e.g., MiniGrid-Empty-5x5-v0 is (40,40,3) while MiniGrid-Empty-8x8-v0 is (64,64,3). So you may need to resize the observations for transfer learning to work with the same DQN architecture.\n",
        "\n",
        "Now try training a DQN (or another method) in one (or multiple) minigrid env(s),and see if that knowledge transfers to another (or multiple other) minigrid env(s).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Qy1rMmg4obbz"
      },
      "source": [
        "## 6 Preference-Based RL (PBRL)\n",
        "\n",
        "PBRL is an exciting sub-area in RL where the traditional reward structure is replaced with human preferences. This setting is very useful in applications where it is difficult to construct a reward function. \n",
        "\n",
        "In the earlier section, we have successfully trained the lunar lander to land safely. Here, the path that the lander follows to land safely can be arbitrary. In this project, using the techniques of PBRL, you will solve the lunar lander problem with an additional requirement that the lander should follow a specially curated path (for example, a straight line path). Following are some of the resources that will help you to get started with this project. \n",
        "\n",
        "**Research papers:**\n",
        "1. Deep Reinforcement Learning\n",
        "from Human Preferences https://papers.nips.cc/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf\n",
        "2. Deep Q-learning from Demonstrations https://arxiv.org/pdf/1704.03732.pdf\n",
        "3. Reward learning from human preferences https://arxiv.org/pdf/1811.06521.pdf\n",
        "4. T-REX https://arxiv.org/pdf/1904.06387.pdf\n",
        "\n",
        "**Code Bases:**\n",
        "1. https://github.com/nottombrown/rl-teacher\n",
        "2. https://github.com/hiwonjoon/ICML2019-TREX\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ptpdnpwGobbz"
      },
      "source": [
        "---\n",
        "# References\n",
        "\n",
        "1. Stable Baselines Framework: https://stable-baselines3.readthedocs.io/en/master/guide/examples.html\n",
        "\n",
        "2. Lunar Lander Environment: https://gym.openai.com/envs/LunarLander-v2/\n",
        "\n",
        "3. OpenAI gym environments: https://gym.openai.com/docs/\n",
        "\n",
        "4. A good reference for introduction to RL: http://incompleteideas.net/book/the-book-2nd.html\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "lunar_lander_group1_atcheke",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}